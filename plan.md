# AI Broker Message Extractor: Project Plan

## 1. Project Goal

The primary goal is to build an AI model that automatically reads unstructured text messages from financial brokers and extracts structured data. This includes details like issuers, coupons, maturities, and bid/offer information for multiple securities within a single message. The model should also classify the overall "type" of the deal.

This project will be developed in phases, starting with a core entity extraction model and then expanding its capabilities.

---

## 2. Core Architecture

We will use a **pre-trained transformer model** from the Hugging Face ecosystem. The architecture will be a **Multi-Task Learning** setup:

-   **Shared Body:** A single transformer model (e.g., DistilBERT, BERT, or a finance-specific variant) will process the input text and generate a contextual representation.
-   **Two "Heads":**
    1.  **Token Classification Head:** This will be used for Named Entity Recognition (NER) to extract the structured details (Issuer, Coupon, etc.). It assigns a label to each word/token in the message.
    2.  **Sequence Classification Head:** This will be used to classify the overall "Deal Type" (`Primary`, `Dealt`, `Switch`, etc.). It assigns a single label to the entire message.

This architecture is efficient as both tasks benefit from the shared language understanding of the model's body.

---

## 3. Development Phases

### Phase 1: Setup and Data Preparation

**This is the most critical phase.** The model's performance is highly dependent on the quality of the training data.

**3.1. Requirements:**
-   Python 3.8+
-   An IDE like VS Code or a Jupyter Notebook environment.
-   Install necessary libraries:
    ```bash
    pip install pandas torch transformers[torch] datasets seqeval
    ```

**3.2. Input Data:**
-   **Source:** `C:\BrokerMsg\MainTrainingData.csv`
-   **Structure:**
    -   Inputs: `Time`, `Dealer`, `Chat`
    -   Labels: `Type`, `Issuer`, `Coupon`, ... `Offer amount` (Columns D-AF)

**3.3. Core Task: Create the Data Preparation Script (`prepare_data.py`)**
The goal is to convert the CSV data into the **IOB (Inside-Outside-Beginning)** format required for NER training.

-   **Script Logic:**
    1.  Load `MainTrainingData.csv` into a `pandas` DataFrame.
    2.  For each row in the DataFrame:
        a. Take the `Chat` message. Use a proper tokenizer from a pre-trained model (e.g., `Distilbert-base-uncased`) to split the message into tokens.
        b. Create a list of `iob_tags`, initially all set to `'O'` (Outside), with the same length as the list of tokens.
        c. For each of the 23 label columns (from `Issuer` to `Offer amount for fourth security`):
            i. If the label cell is not empty, find the corresponding text in the original chat message.
            ii. **Crucially, find the exact tokens** that correspond to this label text.
            iii. Apply the appropriate IOB tags. For example, for the `Issuer` "The Federal Bank", the corresponding tokens would be tagged as `B-Issuer`, `I-Issuer`, `I-Issuer`.
        d. **Recommendation for Multiple Securities:** For simplicity in this first phase, use the same entity label regardless of which security it belongs to (e.g., `B-Issuer` for all issuers). The model can learn to distinguish them from context.
    3.  Store the results. The final output should be a list of dictionaries, where each dictionary contains `tokens` and `iob_tags`. Save this processed data as `training_data.json`.

---

### Phase 2: Training the Baseline NER Model

**Goal:** Train a model that *only* performs the NER task. We will add the deal type classification later.

**4.1. Create the Training Script (`train_ner.py`)**
-   **Script Logic:**
    1.  Load the `training_data.json` file created in Phase 1 using the Hugging Face `datasets` library.
    2.  Perform a train/test split (e.g., 80% for training, 20% for evaluation).
    3.  **Choose a Base Model:** Start with `distilbert-base-uncased`. It's a good balance of performance and speed.
    4.  Load the tokenizer and a `AutoModelForTokenClassification` model from Hugging Face.
    5.  Write a function to tokenize the input texts and align the IOB labels with the new tokens generated by the tokenizer.
    6.  Define `TrainingArguments` (output directory, number of epochs, batch size, etc.).
    7.  Instantiate the `Trainer`, passing it the model, training args, and datasets.
    8.  Call `trainer.train()`.
    9.  After training, call `trainer.save_model("./broker-ner-model")` to save your fine-tuned model.

---

### Phase 3: Evaluation and Prediction

**Goal:** Measure the baseline model's performance and use it for inference.

**5.1. Create the Evaluation Script (`evaluate.py`)**
-   **Script Logic:**
    1.  Load your fine-tuned model and the test dataset.
    2.  Use the `Trainer`'s `evaluate()` method.
    3.  To get detailed, per-entity metrics (Precision, Recall, F1-score), use the `seqeval` library. The `Trainer` can be configured to use this during evaluation.

**5.2. Create a Prediction Script (`predict.py`)**
-   **Script Logic:**
    1.  Load your fine-tuned model from the `./broker-ner-model` directory using `pipeline("ner", model="./broker-ner-model")`.
    2.  Define a new, unseen broker message as a string.
    3.  Pass the message to the pipeline.
    4.  Print the extracted entities to see your model in action.

---

### Phase 4: Iteration and Advanced Model

**Goal:** Implement the human-in-the-loop workflow and upgrade the model to handle deal type classification.

**6.1. Human-in-the-Loop Workflow:**
-   Use your `predict.py` script on new, unlabeled data.
-   Manually review the model's predictions.
-   Correct any errors and determine the true labels.
-   Format this new, corrected data into the IOB format and add it to your `training_data.json` file.
-   Periodically re-run the training script (Phase 2) to improve the model with this new data.

**6.2. Upgrade to Multi-Task Architecture:**
-   **Data Prep:** Your processed data should now also include the `Deal Type` label for each sequence.
-   **Architecture:** You will need to create a custom model class that inherits from the `transformers` base model class (e.g., `DistilBertPreTrainedModel`). This custom class will include both a token classification head and a sequence classification head.
-   **Training:** The training loop will need to be modified to calculate a combined loss from both heads.
-   **Recommendation:** Tackle this *after* you are satisfied with the performance of your baseline NER model. It adds complexity, but will ultimately create a more powerful and integrated solution.
